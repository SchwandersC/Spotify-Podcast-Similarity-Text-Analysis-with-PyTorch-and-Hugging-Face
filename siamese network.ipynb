{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cac2f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0baeb9",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f836aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"C:\\\\Users\\\\caneu\\\\Downloads\\\\embeddings_and_info.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be606ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_pickle(\"C:\\\\Users\\\\caneu\\\\Downloads\\\\embeddings_and_info2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1439877",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, df1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b21d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_pickle('C:\\\\Users\\\\caneu\\\\Downloads\\\\meta_data_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319802d6",
   "metadata": {},
   "source": [
    "### I discovered that descriptions which mention anchor are all typically the same as the podcast provider adds on a little blurb on the end that can mess with similarity scores so I remove these episodes for the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80ba06f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_meta = meta_data[~meta_data['episode_description'].str.contains('anchor', case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ce0e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'rw' is the column in df from which you want to strip '.json'\n",
    "df['episode_id'] = df['episode_id'].str.rstrip('.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "aed3d297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   show_id     50000 non-null  object\n",
      " 1   episode_id  50000 non-null  object\n",
      " 2   transcript  50000 non-null  object\n",
      " 3   embeddings  50000 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "4eb04428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 63610 entries, 0 to 105358\n",
      "Data columns (total 15 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   show_uri                 63610 non-null  object \n",
      " 1   show_name                63610 non-null  object \n",
      " 2   show_description         63610 non-null  object \n",
      " 3   publisher                63610 non-null  object \n",
      " 4   language                 63610 non-null  object \n",
      " 5   rss_link                 63610 non-null  object \n",
      " 6   episode_uri              63610 non-null  object \n",
      " 7   episode_name             63610 non-null  object \n",
      " 8   episode_description      63406 non-null  object \n",
      " 9   duration                 63610 non-null  float64\n",
      " 10  show_filename_prefix     63610 non-null  object \n",
      " 11  episode_filename_prefix  63610 non-null  object \n",
      " 12  name_embeds              63610 non-null  object \n",
      " 13  name_embeds1             63610 non-null  object \n",
      " 14  description_embeds       63610 non-null  object \n",
      "dtypes: float64(1), object(14)\n",
      "memory usage: 7.8+ MB\n"
     ]
    }
   ],
   "source": [
    "new_meta.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd42eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'episode_id' is the common column in df and 'episode_filename_prefix' is the common column in meta_data\n",
    "merged_df = pd.merge(df, new_meta, left_on='episode_id', right_on='episode_filename_prefix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de3ba5",
   "metadata": {},
   "source": [
    "## Merge meta data descriptions and name embeddings with transcript embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "8298f3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 27687 entries, 0 to 27686\n",
      "Data columns (total 19 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   show_id                  27687 non-null  object \n",
      " 1   episode_id               27687 non-null  object \n",
      " 2   transcript               27687 non-null  object \n",
      " 3   embeddings               27687 non-null  object \n",
      " 4   show_uri                 27687 non-null  object \n",
      " 5   show_name                27687 non-null  object \n",
      " 6   show_description         27687 non-null  object \n",
      " 7   publisher                27687 non-null  object \n",
      " 8   language                 27687 non-null  object \n",
      " 9   rss_link                 27687 non-null  object \n",
      " 10  episode_uri              27687 non-null  object \n",
      " 11  episode_name             27687 non-null  object \n",
      " 12  episode_description      27600 non-null  object \n",
      " 13  duration                 27687 non-null  float64\n",
      " 14  show_filename_prefix     27687 non-null  object \n",
      " 15  episode_filename_prefix  27687 non-null  object \n",
      " 16  name_embeds              27687 non-null  object \n",
      " 17  name_embeds1             27687 non-null  object \n",
      " 18  description_embeds       27687 non-null  object \n",
      "dtypes: float64(1), object(18)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7774507",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = np.array(merged_df['embeddings'].tolist())\n",
    "name_embeds1_array = np.array(merged_df['name_embeds1'].tolist())\n",
    "description_embeds_array = np.array(merged_df['description_embeds'].tolist())\n",
    "\n",
    "# Concatenate the arrays along axis=2 (third dimension)\n",
    "combined_embeddings = np.concatenate([embeddings_array, name_embeds1_array, description_embeds_array], axis=2)\n",
    "\n",
    "# Reshape the resulting 3D tensor to a 2D array for cosine similarity calculation\n",
    "combined_embeddings_2d = combined_embeddings.reshape(-1, combined_embeddings.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e3fdcf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27687, 2304)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_embeddings_2d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021da767",
   "metadata": {},
   "source": [
    "## chose to combine embeddings by concatenation. Other methods also exist like sum and average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "578dd574",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_test = combined_embeddings_2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "736a003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_np = np.array([np.array(e).flatten() for e in embeddings_test])\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "normalized_embedding = normalize(embeddings_np, norm='l2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ec70b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27687, 2304)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46606177",
   "metadata": {},
   "source": [
    "# Normalized embeddings after flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "93767672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sims = cosine_similarity(normalized_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee889a8",
   "metadata": {},
   "source": [
    "# Similarity with Siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "04cdcd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "1e9c5ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating embeddings and similarity matrix\n",
    "embeddings = normalized_embedding[:2000]\n",
    "#[:800]\n",
    "similarity_matrix = sims[:2000]\n",
    "#[:800]\n",
    "#ensure each embedding has same shape\n",
    "target_embedding_shape = (2304,)\n",
    "embeddings_resized = [np.resize(embedding, target_embedding_shape) for embedding in embeddings]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "27bfc000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_embeddings, test_embeddings, train_similarity_matrix, test_similarity_matrix = train_test_split(\n",
    "    embeddings_resized, similarity_matrix, test_size=0.2, random_state=2024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "834b4769",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "# Custom Dataset class\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, embeddings, similarity_matrix):\n",
    "        self.embeddings = embeddings\n",
    "        self.similarity_matrix = similarity_matrix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.similarity_matrix[idx]\n",
    "\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim=32, hidden_dim=128, dropout_prob=0.5):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        self.shared_embedding = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = torch.relu(self.shared_embedding(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        l1_distance = torch.abs(output1 - output2)\n",
    "        output = torch.sigmoid(self.fc3(l1_distance))\n",
    "        return output\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_siamese_model(model, train_dataloader, num_epochs=10, learning_rate=0.001, device='cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        # Use tqdm to create a progress bar for the outer loop\n",
    "        with tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as t:\n",
    "            for i, (data_i, target_i) in enumerate(t):\n",
    "                for j, (data_j, target_j) in enumerate(train_dataloader):\n",
    "                    if i != j:\n",
    "                        data_i, target_i = data_i.to(device), target_i.to(device)\n",
    "                        data_j, target_j = data_j.to(device), target_j.to(device)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        output_i = model(data_i[:, 0], data_i[:, 1]).squeeze()\n",
    "                        \n",
    "                        #print(data_j[:, 0].shape, data_j[:, 1].shape)\n",
    "                        output_j = model(data_j[:, 0], data_j[:, 1]).squeeze()\n",
    "\n",
    "                        loss_i = criterion(output_i, target_i)\n",
    "                        loss_j = criterion(output_j, target_j)\n",
    "\n",
    "                        loss = (loss_i + loss_j) / 2\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        # Update the progress bar description with the latest loss\n",
    "                        t.set_postfix(loss=loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Final Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_siamese_model(model, test_dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    mse_loss = nn.MSELoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data[:, 0], data[:, 1]).squeeze()\n",
    "            loss = mse_loss(output, target)\n",
    "            \n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            total_samples += data.size(0)\n",
    "\n",
    "    average_loss = total_loss / total_samples\n",
    "    print(f\"Test MSE Loss: {average_loss}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9d893",
   "metadata": {},
   "source": [
    "### Performed many iterations on full data and subsets to experimentally determine the best hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "5e381a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/50 [00:00<?, ?batch/s]C:\\Users\\caneu\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 27687])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1/10: 100%|██████████| 50/50 [00:48<00:00,  1.04batch/s, loss=0.000586]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Final Loss: 0.0005863613914698362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 50/50 [00:46<00:00,  1.07batch/s, loss=0.000914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Final Loss: 0.0009144514333456755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 50/50 [00:47<00:00,  1.05batch/s, loss=0.000718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Final Loss: 0.0007176126819103956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 50/50 [00:47<00:00,  1.06batch/s, loss=0.00058] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Final Loss: 0.000579961109906435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 50/50 [00:49<00:00,  1.02batch/s, loss=0.000423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Final Loss: 0.00042324449168518186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 50/50 [00:49<00:00,  1.01batch/s, loss=0.000529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Final Loss: 0.0005294568836688995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 50/50 [00:49<00:00,  1.01batch/s, loss=0.000531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Final Loss: 0.0005312708672136068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 50/50 [00:49<00:00,  1.01batch/s, loss=0.000398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Final Loss: 0.0003979144967161119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 50/50 [00:49<00:00,  1.01batch/s, loss=0.00044] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Final Loss: 0.0004397771554067731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 50/50 [00:49<00:00,  1.01batch/s, loss=0.000427]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Final Loss: 0.0004269569180905819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = SiameseDataset(train_embeddings, train_similarity_matrix)\n",
    "test_dataset = SiameseDataset(test_embeddings, test_similarity_matrix)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# Create and train the Siamese model\n",
    "siamese_model = SiameseNetwork()\n",
    "train_siamese_model(siamese_model, train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "cb91bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE Loss: 0.000627488077346546\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last = True)\n",
    "evaluate_siamese_model(siamese_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "ec5a45c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embedding_shape = 32\n",
    "#number below in new_embedding code is the embedding index for the similarity\n",
    "\n",
    "all_embeddings = [np.resize(embedding, target_embedding_shape) for embedding in normalized_embedding]\n",
    "all_embeddings = normalize(all_embeddings, norm='l2')\n",
    "new_embedding = all_embeddings[1400]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca40705f",
   "metadata": {},
   "source": [
    "## Had to resize embeddings in order to find most similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "ad96594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_most_similar_embeddings(new_embedding, all_embeddings, siamese_model, device='cpu'):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    siamese_model.eval()\n",
    "\n",
    "    # Convert new_embedding to torch tensor (assuming it's a NumPy array)\n",
    "    new_embedding = torch.tensor(new_embedding, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Compute similarity scores for each embedding in all_embeddings\n",
    "    similarity_scores = []\n",
    "    with torch.no_grad():\n",
    "        for index, candidate_embedding in enumerate(all_embeddings):\n",
    "            candidate_embedding = torch.tensor(candidate_embedding, dtype=torch.float32).to(device)\n",
    "\n",
    "            # Compute similarity score using the Siamese network\n",
    "            similarity_score = siamese_model(new_embedding, candidate_embedding).item()\n",
    "            similarity_scores.append((index, similarity_score))\n",
    "\n",
    "    # Rank embeddings based on similarity scores\n",
    "    ranked_embeddings = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "0798bc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20413543 0.157709   0.20493902 0.1179009  0.23180876 0.15412084\n",
      " 0.15055615 0.26148681 0.12425503 0.12514287 0.15591911 0.14552698\n",
      " 0.09738209 0.1690598  0.15691232 0.21368616 0.18639392 0.11575423\n",
      " 0.1696905  0.23380276 0.28433459 0.13568801 0.15041693 0.27849924\n",
      " 0.16959643 0.21558517 0.11130428 0.11497336 0.11964039 0.0865617\n",
      " 0.2201813  0.14801854]\n",
      "Rank 1: Similarity Score = 0.9214, Index = 21872, Embedding = [ 0.14415032  0.17037932  0.30246338  0.10554048  0.15577215  0.18374927\n",
      "  0.13197063  0.28999314  0.09131534 -0.05203622  0.15121206  0.01859102\n",
      "  0.06332877  0.36348518  0.16999856  0.24653199  0.11396783  0.14490973\n",
      "  0.04528691  0.19597573  0.20218719  0.20663859  0.11656036  0.21562957\n",
      "  0.20373682  0.14003292  0.08962645  0.15911535  0.08444207  0.21075596\n",
      "  0.20120348  0.15282271]\n",
      "Rank 2: Similarity Score = 0.9210, Index = 9244, Embedding = [0.18175234 0.16592793 0.30060937 0.15962347 0.11377095 0.2603173\n",
      " 0.16744441 0.25871898 0.11980185 0.1167338  0.05810829 0.06544641\n",
      " 0.15514293 0.26161805 0.06610606 0.20317648 0.13083994 0.14295002\n",
      " 0.08177659 0.33846471 0.24416473 0.11180639 0.15055428 0.16253128\n",
      " 0.18627126 0.16133675 0.17185936 0.19120875 0.08706757 0.10299622\n",
      " 0.19720878 0.1009965 ]\n",
      "Rank 3: Similarity Score = 0.9210, Index = 9222, Embedding = [0.31826241 0.05600051 0.28088942 0.14403544 0.23937249 0.22610042\n",
      " 0.14740437 0.3727378  0.08448309 0.0826566  0.09836002 0.08422418\n",
      " 0.12546551 0.28337179 0.07162578 0.19537845 0.06793959 0.10859913\n",
      " 0.19698336 0.15150498 0.1335231  0.0565023  0.08714567 0.18125188\n",
      " 0.21728889 0.08613926 0.04588427 0.23427518 0.18367715 0.03519783\n",
      " 0.22285334 0.14445061]\n",
      "Rank 4: Similarity Score = 0.9207, Index = 4769, Embedding = [0.26421625 0.17853143 0.30312013 0.07425331 0.11618587 0.13683234\n",
      " 0.28528518 0.3124281  0.11512021 0.10822309 0.14940664 0.02633366\n",
      " 0.15303709 0.22763768 0.02136482 0.18111985 0.11853872 0.08489004\n",
      " 0.12282366 0.34505781 0.19208222 0.12066094 0.11947323 0.13691265\n",
      " 0.13230718 0.12273026 0.06876896 0.21570626 0.13715458 0.12234182\n",
      " 0.22549014 0.15775324]\n",
      "Rank 5: Similarity Score = 0.9207, Index = 23785, Embedding = [0.26513884 0.11502302 0.38400215 0.15264154 0.19335862 0.07726972\n",
      " 0.35175441 0.29435812 0.09640108 0.14484975 0.09851123 0.05453427\n",
      " 0.04881341 0.21453671 0.10977006 0.24288392 0.06496669 0.21703821\n",
      " 0.11814695 0.25448107 0.17027713 0.05585298 0.0794272  0.1508163\n",
      " 0.13303994 0.12558698 0.03924842 0.19917877 0.08684412 0.04813332\n",
      " 0.2198189  0.07414201]\n"
     ]
    }
   ],
   "source": [
    "result = find_most_similar_embeddings(new_embedding, all_embeddings, siamese_model)\n",
    "\n",
    "# Print the top N most similar embeddings\n",
    "top_n = 5\n",
    "print(new_embedding)\n",
    "for i, (index, similarity_score) in enumerate(result[:top_n], 1):\n",
    "    print(f\"Rank {i}: Similarity Score = {similarity_score:.4f}, Index = {index}, Embedding = {all_embeddings[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "fc11fa81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For our final episode, were talking about healing.\\xa0'"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['episode_description'][1400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "5c6aa5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction to Rastafari '"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['episode_description'][21872]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f8af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
